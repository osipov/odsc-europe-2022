{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qt_irrBWLLB"
   },
   "source": [
    "# Automatic Differentiation Explained \n",
    "### using Python with an application to linear regression solved using gradient descent\n",
    "\n",
    "by <a href=\"mailto:carl.osipov@gmail.com\"><b>Carl Osipov</b></a>, based on the materials from his <a href=\"https://bit.ly/mlops-book\"><b>\"MLOps Engineering at Scale\"</b></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iURTnb1cXHy"
   },
   "source": [
    "In this notebook, you can learn about:\n",
    "\n",
    "* what is a tensor in deep learning\n",
    "* how are scalar and array tensors created and used in PyTorch\n",
    "* how PyTorch `autograd` framework can be used for linear regression\n",
    "* how reverse mode automatic differentiation can be implemented using Python\n",
    "* the design principles behind PyTorch `autograd` application programming interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdMn-cuZSsbp"
   },
   "source": [
    "# Tensor is the core deep learning data structure (container)\n",
    "\n",
    "![title](https://github.com/osipov/odsc-europe-2022/raw/main/img/sbHDGMs.png)\n",
    "\n",
    "* like in NumPy, a tensor is a multi-dimensional array. Unlike in NumPy, a tensor in deep learning supports differentiation\n",
    "* the number of indices needed to access a value in the tensor is equal to the tensor dimension (rank)\n",
    "* the number of values along every dimension (tensor shape) is constant, in other words, you can't have a matrix with 42 columns in the 1st row and 3 columns in the 2nd row (unlike \"ragged\" tensors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a `Scalar` class, a rank 0 tensor, with support for `__repr__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:    \n",
    "    def __init__(self, val):\n",
    "        self.val = float(val)\n",
    "        self.grad = float(0)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value: {self.val}, Gradient: {self.grad}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that you can create a `Scalar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value: 42.0, Gradient: 0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scalar(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add support for `__add__`, and `__mul__` methods to the `Scalar` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:    \n",
    "    def __init__(self, val):\n",
    "        self.val = float(val)\n",
    "        self.grad = float(0)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value: {self.val}, Gradient: {self.grad}\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        result = Scalar(self.val + other.val)\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        result = Scalar(self.val * other.val)\n",
    "        return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that you can do basic arithmetic with a `Scalar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value: 42.0, Gradient: 0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( Scalar(3) + Scalar(3) ) * Scalar(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a placeholder function in `Scalar` to do autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Quick Python refresher `lambda: None` creates a function that returns `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.foo()>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo():\n",
    "    return None\n",
    "\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>()>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>()>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = lambda: None\n",
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:    \n",
    "    def __init__(self, val):\n",
    "        self.val = float(val)\n",
    "        self.grad = float(0)\n",
    "        \n",
    "        #TODO: include a noop self.backward definition here\n",
    "        self.backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value: {self.val}, Gradient: {self.grad}\" \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        result = Scalar(self.val + other.val)\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        result = Scalar(self.val * other.val)        \n",
    "        return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation (finding derivatives) from first principles\n",
    "\n",
    "**derivative:** a function specifying the slope of a line tangent to a target function for some value(s)\n",
    "\n",
    "For instance, if the vertical dimension on the following coordinate plane is defined using $ y = f(x) $ where $ f(x) = x^2 $ (shown below)\n",
    "\n",
    "we will use <a href=\"https://en.wikipedia.org/wiki/Leibniz%27s_notation\">Leibniz's notation</a> $ \\frac{\\partial y}{\\partial x} = 2x $ for the derivative of $ y $ with respect to $ x $ \n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "    <a href=\"https://www.geogebra.org/classic/tfhqjsmw\"><img src=\"https://github.com/osipov/odsc-europe-2022/raw/main/img/y-x-squared.png\" width=\"600\"/></a>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick check:** Given $ y = f(x) = x $ what's the expression for $ \\frac{\\partial y}{\\partial x} $. Hint below.\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://github.com/osipov/odsc-europe-2022/raw/main/img/y-x.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE3wdfUk83Wc"
   },
   "source": [
    "## Understanding reverse mode accumulating automatic differentiation\n",
    "\n",
    "Automatic differentiation (autodiff) is different from:\n",
    "\n",
    "* **symbolic differentiation** taught in calculus classes or implemented in some computer math packages (e.g. Mathematica) that attempt to derive a general symbolic expression of the differentiated function for arbitrary values requiring more computation and memory than autodiff. \n",
    "\n",
    "* **numeric differentiation**, which is based on an approximation of $   \\lim_{\\epsilon \\to 0} \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}$, which can be numerically unstable at the extreme values of the differentiated functions and  accumulate small errors introduced by floating point number approximations to real numbers.\n",
    "\n",
    "\n",
    "Autodiff finds a **derivative of a function at specific values of the function's input variables** one at a time, with a computation complexity $ O(n) $ where `n` is the number of the mathematical operations used by the differentiated function. Notice that $ O(n) $ holds only when the number of outputs of the function is fewer than the number of inputs.\n",
    "\n",
    "**NOTE:** Going forward I'll use gradient interchangably with derivative. The specific distinction has to do with calculation of derivatives of tensor variables vs. a derivative of a simple variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise!\n",
    "### `Scalar` already does autodiff for a trivial (base) case. Let me show you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqeyPMtaKQ1A"
   },
   "source": [
    "### Create a `Scalar` instance for `x = 2.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "UdZXolFUKQ1B"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value: 2.0, Gradient: 0.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Scalar(2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpCVxuQcKQ1E"
   },
   "source": [
    "### Define a function $ y $ using  `y = x` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "Bu8de4X4KQ1E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value: 2.0, Gradient: 0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "* **Given:** $ y = x $\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://github.com/osipov/odsc-europe-2022/raw/main/img/y_eq_x.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform autodiff on   `y = x` \n",
    "* Goal is to find $ \\frac{\\partial y}{ \\partial x} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Zero out the desired gradients \n",
    "\n",
    "**NOTE:** This is due to accumulation, explained shortly\n",
    "* Use floating point values\n",
    "\n",
    "Here the desired gradient $ \\frac{\\partial y}{ \\partial x} $, so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad = 0.\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qq9ScpmKQ1H"
   },
   "source": [
    "### Step 2. Prepare for a call to reverse mode accumulating automatic differentiation via `backward`\n",
    "* Use floating point values\n",
    "\n",
    "How to initialize $ \\frac{\\partial y}{ \\partial y} $ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "WAHVfnC_KQ1I"
   },
   "outputs": [],
   "source": [
    "y.grad = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Call `backward` on `y`\n",
    "\n",
    "* Compute an answer to the question: Given that  $ x $  changes by $ \\partial x $, what's the ratio of a resulting change to `y` (here `dy`) to `dx`, or $ \\frac{\\partial y}{ \\partial x} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRbBgulCKQ1L"
   },
   "source": [
    "* check that $ \\frac{\\partial y}{ \\partial x} = \\frac{1}{1} = 1.0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Find out the gradient(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "tBimRd4cKQ1L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x\n",
    "x.grad = 0.\n",
    "y.grad = 1.\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC5POt4UKQ1P"
   },
   "source": [
    "* **Self-check:** Why the did the implementation return the correct answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement autodiff for `Scalar` addition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "* **Given:** $ z = c = a + b $\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://github.com/osipov/odsc-europe-2022/raw/main/img/a_pl_b_eq_c_eq_z.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-JVg1nkKQ1P"
   },
   "source": [
    "## Modify the` __add__` function to include  `backward` support\n",
    "* **hint:** given $ y = x + z $, $ \\frac{\\partial y}{ \\partial x} = 1.0 $ and $ \\frac{\\partial y}{ \\partial z} = 1.0 $\n",
    "\n",
    "\n",
    "* **hint:** don't forget the recursive `backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "ALT7TH5LKQ1Q"
   },
   "outputs": [],
   "source": [
    "class Scalar:    \n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.grad = 0.\n",
    "        self.backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value: {self.val}, Gradient: {self.grad}\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        result = Scalar(self.val + other.val)\n",
    "\n",
    "        def backward():        \n",
    "\n",
    "          self.grad += result.grad\n",
    "          other.grad += result.grad\n",
    "\n",
    "          self.backward()\n",
    "          other.backward()\n",
    "            \n",
    "        result.backward = backward\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        result = Scalar(self.val * other.val)\n",
    "        return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoQ8FV8DKQ1S"
   },
   "source": [
    "  ## Define `y = x + x + x` i.e. $y = 3 * x$\n",
    "\n",
    "* evaluate for x = 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "s88Zk_6cKQ1T"
   },
   "outputs": [],
   "source": [
    "x = Scalar(42.)\n",
    "y = (x + x) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx3zDy3dKQ1V"
   },
   "source": [
    "## Prepare for and run the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "PKtdBqCIKQ1W"
   },
   "outputs": [],
   "source": [
    "x.grad = 0.\n",
    "y.grad = 1.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw8otmYLKQ1Z"
   },
   "source": [
    "* check that $ \\frac{\\partial y}{ \\partial x} = 3.0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "PaZsCpANKQ1Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Self-check:** Are you clear now why you needed accumulation `+=` in `backward`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement autodiff for `Scalar` multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "\n",
    "* **Given:** $ z = c = a * b $\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://github.com/osipov/odsc-europe-2022/raw/main/img/a_ml_b_eq_c_eq_z.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA2ggcfHKQ1c"
   },
   "source": [
    "## Implement `backward` support in the` __mul__` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "PuR5VHEBKQ1d"
   },
   "outputs": [],
   "source": [
    "class Scalar:    \n",
    "    def __init__(self, val):\n",
    "        self.val = val\n",
    "        self.grad = 0.\n",
    "        self.backward = lambda: None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value: {self.val}, Gradient: {self.grad}\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        result = Scalar(self.val + other.val)\n",
    "        def backward():        \n",
    "\n",
    "          self.grad += result.grad\n",
    "          other.grad += result.grad\n",
    "\n",
    "\n",
    "          self.backward()\n",
    "          other.backward()\n",
    "            \n",
    "        result.backward = backward\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        result = Scalar(self.val * other.val)\n",
    "\n",
    "        def backward():\n",
    "\n",
    "          self.grad += other.val * result.grad\n",
    "          other.grad += self.val * result.grad\n",
    "\n",
    "          self.backward()\n",
    "          other.backward()\n",
    "        \n",
    "        result.backward = backward\n",
    "\n",
    "        return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReYBUarZKQ1f"
   },
   "source": [
    "## Use `y = x^3 + 2*x` for `x = 4.0`\n",
    "* **hint:** recall that $ x^3 = x * x * x $\n",
    "* **NOTE:** parentheses are just for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "0VnU35HOKQ1g"
   },
   "outputs": [],
   "source": [
    "x = Scalar(4.)\n",
    "\n",
    "y = ( x * x * x ) + x + x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr15Uw_cKQ1i"
   },
   "source": [
    "* given $ y = x^3 + 2x $ the analytical solution to $ \\frac{\\partial y}{ \\partial x} = 3x^2+2 $\n",
    "* check that your implementation of `Scalar` returns the correct value of $ \\frac{\\partial y}{ \\partial x} $ when $ x = 4.0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "um4aAcr9KQ1i"
   },
   "outputs": [],
   "source": [
    "x.grad = 0.\n",
    "y.grad = 1.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "ivKVjg0tKQ1n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBDfSGfcKQ1p"
   },
   "source": [
    "## Apply `Scalar` to linear regression\n",
    "* set the random seed to `42`\n",
    "* set the epochs to `100`\n",
    "* set the learning rate to `0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "import random\n",
    "random.seed(a = SEED, version = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* randomly init the model parameter `w` using uniform distribution [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Iy4Phpx4KQ1p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value: 0.2788535969157675, Gradient: 0.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Scalar(random.uniform(-1, 1))\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIYhatJTKQ1s"
   },
   "source": [
    "## Make linear regression data\n",
    "* although the following is easier with NumPy, let's use pure Python to create a list of `Scalar` instances in a range `[-5,5)` with a step of `0.1`\n",
    "* here's a reference scatter plot for what the data will look like\n",
    "\n",
    "<div>\n",
    "    <img src=\"img/linreg.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "txdVmceyKQ1s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value: -5.0, Gradient: 0.0,\n",
       " Value: -4.9, Gradient: 0.0,\n",
       " Value: -4.800000000000001, Gradient: 0.0,\n",
       " Value: -4.7, Gradient: 0.0,\n",
       " Value: -4.6000000000000005, Gradient: 0.0]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [Scalar(i * 0.1) for i in range(-50, 50)]\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "jF9WrMD4qqxP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value: -34.207855259050206, Gradient: 0.0,\n",
       " Value: -34.17448172429675, Gradient: 0.0,\n",
       " Value: -33.326400993441624, Gradient: 0.0,\n",
       " Value: -31.28992890553664, Gradient: 0.0,\n",
       " Value: -33.13805122143324, Gradient: 0.0]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = Scalar(7)\n",
    "y = [W * X[i] + Scalar(random.gauss(mu = 0, sigma = 1)) for i in range(len(X))]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHnQPRuvKQ1v"
   },
   "source": [
    "## Implement a `forward` function using `w`\n",
    "* **hint:** the function should return $ w * X $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "1rD8GNipKQ1v"
   },
   "outputs": [],
   "source": [
    "def forward(X, w):\n",
    "  y_pred = [w * X[i] for i in range(len(X))]\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHeMP08VKQ1x"
   },
   "source": [
    "## Implement the mean squared error calculation\n",
    "* **hint:** Python `sum` can use a starter value as the 2nd argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "BiW6fdXIKQ1x"
   },
   "outputs": [],
   "source": [
    "def mse(y, y_pred):\n",
    "  error = [y[i] + Scalar(-1.0) * y_pred[i] for i in range(len(y))]\n",
    "  se = [error[i] * error[i] for i in range(len(error))]\n",
    "  mse = Scalar(1./len(error)) * sum(se, Scalar(0))\n",
    "  return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5qi1_frKQ1z"
   },
   "source": [
    "## Confirm that gradient descent recovers the value used to generate `y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "v9ztA1u7KQ10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value: 6.984187710500422, Gradient: -3.735900477863652e-14"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(EPOCHS):\n",
    "  y_pred = forward(X, w)\n",
    "  loss = mse(y, y_pred)\n",
    "\n",
    "  w.grad = 0.\n",
    "  loss.grad = 1.\n",
    "  loss.backward()\n",
    "\n",
    "  w.val -= LEARNING_RATE * w.grad\n",
    "  \n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2022 CounterFactual.AI LLC. Licensed under GNU General Public License v3.0\n",
    "Permissions of this strong copyleft license are conditioned on making available complete source code of licensed works and modifications, which include larger works using a licensed work, under the same license. Copyright and license notices must be preserved. Contributors provide an express grant of patent rights."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Solution_Autodiff_Algorithm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
